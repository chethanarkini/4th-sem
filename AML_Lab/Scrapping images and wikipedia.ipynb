{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4391a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b17df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Wikipedia\n",
      "Paragraphs:\n",
      "\n",
      "Save your favorite articles to read offline, sync your reading lists across devices and customize your reading experience with the official Wikipedia app.\n",
      "\n",
      "\n",
      "This page is available under the Creative Commons Attribution-ShareAlike License\n",
      "Terms of Use\n",
      "Privacy Policy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "url = 'https://www.wikipedia.org'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract specific elements\n",
    "title = soup.title.text\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Print the extracted data\n",
    "print(\"Title:\", title)\n",
    "print(\"Paragraphs:\")\n",
    "for p in paragraphs:\n",
    "    print(p.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42be8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping and storing data in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf846c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to top_movies.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract specific elements\n",
    "movie_rows = soup.select('tbody.lister-list tr')\n",
    "\n",
    "# Create and write data to CSV file\n",
    "with open('top_movies.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Rank', 'Title', 'Year'])\n",
    "    for row in movie_rows:\n",
    "        rank = row.select_one('.posterColumn span[name=\"rk\"]').text.strip()\n",
    "        title = row.select_one('.titleColumn a').text.strip()\n",
    "        year = row.select_one('.titleColumn span.secondaryInfo').text.strip('()')\n",
    "        writer.writerow([rank, title, year])\n",
    "\n",
    "print(\"Data has been scraped and saved to top_movies.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64198793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                     Title  Year\n",
       "0   NaN  The Shawshank Redemption  1994\n",
       "1   NaN             The Godfather  1972\n",
       "2   NaN           The Dark Knight  2008\n",
       "3   NaN     The Godfather Part II  1974\n",
       "4   NaN              12 Angry Men  1957"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('top_movies.csv', encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4acad79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scrapping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14b6cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have been downloaded and saved\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "url = 'https://www.pexels.com/popular-photos'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all image tags\n",
    "image_tags = soup.find_all('img', class_='photo-item__img')\n",
    "\n",
    "# Download and save images\n",
    "for index, img in enumerate(image_tags, start=1):\n",
    "    img_url = img['src']\n",
    "    img_name = f\"image_{index}\"\n",
    "    urllib.request.urlretrieve(img_url, f\"{hello}.jpg\")\n",
    "\n",
    "print(\"Images have been downloaded and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33b277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
